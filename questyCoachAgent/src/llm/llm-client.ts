/**
 * LLM Client
 * OpenRouterë¥¼ í†µí•œ í†µí•© LLM í˜¸ì¶œ
 * - GPT-5 Nano: ì˜ë„ ë¶„ë¥˜ (Fast, <500ms)
 * - Claude Haiku 4.5: ì¼ìƒ ì½”ì¹­ (Balanced)
 * - Gemini 3 Flash: ë³µì¡í•œ ë¶„ì„ + Vision
 */

import type { ModelId, ModelConfig } from '../types/agent.js';

// ëª¨ë¸ ì„¤ì •
export const MODEL_CONFIGS: Record<ModelId, ModelConfig> = {
  'gpt-5-nano': {
    id: 'gpt-5-nano',
    provider: 'openai',
    maxTokens: 256,
    temperature: 0.3,
    purpose: 'Intent classification, simple responses',
  },
  'claude-4.5-haiku': {
    id: 'claude-4.5-haiku',
    provider: 'anthropic',
    maxTokens: 2048,
    temperature: 0.7,
    purpose: 'Daily coaching, emotional support',
  },
  'gemini-3-flash': {
    id: 'gemini-3-flash',
    provider: 'google',
    maxTokens: 4096,
    temperature: 0.5,
    purpose: 'Complex analysis, vision tasks',
  },
};

// OpenRouter ëª¨ë¸ ë§¤í•‘
const OPENROUTER_MODEL_MAP: Record<ModelId, string> = {
  'gpt-5-nano': 'openai/gpt-4o-mini',
  'claude-4.5-haiku': 'anthropic/claude-haiku-4.5',  // Claude Haiku 4.5 (ì‹ ê·œ ëª¨ë¸)
  'gemini-3-flash': 'google/gemini-2.0-flash-001',
};

export interface LLMMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export interface LLMResponse {
  content: string;
  model: ModelId;
  usage: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  latencyMs: number;
}

export interface LLMClientConfig {
  apiKey?: string;  // OpenRouter API Key
  timeout?: number;
  retryAttempts?: number;
}

const DEFAULT_CONFIG: LLMClientConfig = {
  timeout: 30000,
  retryAttempts: 3,
};

export class LLMClient {
  private config: LLMClientConfig;
  private requestCount: number = 0;

  constructor(config: Partial<LLMClientConfig> = {}) {
    this.config = { ...DEFAULT_CONFIG, ...config };

    // í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ
    if (!this.config.apiKey) {
      this.config.apiKey = process.env.OPENROUTER_API_KEY ?? '';
    }
  }

  /**
   * LLM í˜¸ì¶œ (ëª¨ë“  ëª¨ë¸ OpenRouter ì‚¬ìš©)
   */
  async call(params: {
    model: ModelId;
    messages: LLMMessage[];
    maxTokens?: number;
    temperature?: number;
  }): Promise<LLMResponse> {
    const { model, messages, maxTokens, temperature } = params;
    const modelConfig = MODEL_CONFIGS[model];
    const startTime = Date.now();

    // API í‚¤ í™•ì¸ (ëŸ°íƒ€ì„ì— í™˜ê²½ë³€ìˆ˜ ì¬í™•ì¸)
    const apiKey = this.config.apiKey || process.env.OPENROUTER_API_KEY || '';

    // API í‚¤ ì—†ìœ¼ë©´ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ
    if (!apiKey) {
      console.warn('[LLMClient] No API key found, using simulation mode');
      return this.simulateResponse(model, messages, startTime);
    }

    // API í‚¤ê°€ ìˆìœ¼ë©´ configì—ë„ ì €ì¥ (ìºì‹±)
    if (!this.config.apiKey && apiKey) {
      this.config.apiKey = apiKey;
    }

    const openRouterModel = OPENROUTER_MODEL_MAP[model];

    try {
      const response = await this.fetchWithRetry({
        url: 'https://openrouter.ai/api/v1/chat/completions',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${this.config.apiKey}`,
          'HTTP-Referer': 'https://questycoach.app',
          'X-Title': 'QuestyCoach Agent',
        },
        body: {
          model: openRouterModel,
          messages,
          max_tokens: maxTokens ?? modelConfig.maxTokens,
          temperature: temperature ?? modelConfig.temperature,
        },
      });

      const data = await response.json();
      this.requestCount++;

      return {
        content: data.choices?.[0]?.message?.content ?? '',
        model,
        usage: {
          promptTokens: data.usage?.prompt_tokens ?? 0,
          completionTokens: data.usage?.completion_tokens ?? 0,
          totalTokens: data.usage?.total_tokens ?? 0,
        },
        latencyMs: Date.now() - startTime,
      };
    } catch (error) {
      console.error(`[LLMClient] Error calling ${model}:`, error);
      throw error;
    }
  }

  /**
   * 3-Level Router ê¸°ë°˜ ë™ì  ëª¨ë¸ ì„ íƒ í˜¸ì¶œ
   */
  async callWithComplexity(params: {
    messages: LLMMessage[];
    complexity: number; // 0-1
    maxTokens?: number;
  }): Promise<LLMResponse> {
    const { complexity } = params;

    // ë³µì¡ë„ ê¸°ë°˜ ëª¨ë¸ ì„ íƒ
    let model: ModelId;
    if (complexity < 0.3) {
      model = 'gpt-5-nano';
    } else if (complexity < 0.6) {
      model = 'claude-4.5-haiku';
    } else {
      model = 'gemini-3-flash';
    }

    return this.call({
      model,
      messages: params.messages,
      maxTokens: params.maxTokens,
    });
  }

  /**
   * Vision ê¸°ëŠ¥ (Gemini ì „ìš©)
   */
  async callWithVision(params: {
    messages: LLMMessage[];
    imageUrl?: string;
    imageBase64?: string;
  }): Promise<LLMResponse> {
    // Visionì€ í•­ìƒ Gemini ì‚¬ìš©
    return this.call({
      model: 'gemini-3-flash',
      messages: params.messages,
    });
  }

  /**
   * ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ fetch
   */
  private async fetchWithRetry(params: {
    url: string;
    headers: Record<string, string>;
    body: Record<string, unknown>;
  }): Promise<Response> {
    const { url, headers, body } = params;
    let lastError: Error | null = null;

    for (let attempt = 1; attempt <= (this.config.retryAttempts ?? 3); attempt++) {
      try {
        const response = await fetch(url, {
          method: 'POST',
          headers,
          body: JSON.stringify(body),
          signal: AbortSignal.timeout(this.config.timeout ?? 30000),
        });

        if (!response.ok) {
          const errorText = await response.text();
          throw new Error(`HTTP ${response.status}: ${errorText}`);
        }

        return response;
      } catch (error) {
        lastError = error as Error;
        console.warn(`[LLMClient] Attempt ${attempt} failed:`, error);

        // ì¬ì‹œë„ ì „ ëŒ€ê¸° (exponential backoff)
        if (attempt < (this.config.retryAttempts ?? 3)) {
          await this.sleep(Math.pow(2, attempt) * 500);
        }
      }
    }

    throw lastError ?? new Error('Unknown error');
  }

  /**
   * ì‹œë®¬ë ˆì´ì…˜ ì‘ë‹µ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
   */
  private simulateResponse(
    model: ModelId,
    messages: LLMMessage[],
    startTime: number
  ): LLMResponse {
    const lastMessage = messages[messages.length - 1]?.content ?? '';
    const isKorean = /[ê°€-í£]/.test(lastMessage);

    // ëª¨ë¸ë³„ ì‹œë®¬ë ˆì´ì…˜ ì‘ë‹µ
    const responses: Record<ModelId, string> = {
      'gpt-5-nano': isKorean
        ? 'ë„¤, ì´í•´í–ˆìŠµë‹ˆë‹¤. ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤!'
        : 'Got it! I\'ll help you with that.',
      'claude-4.5-haiku': isKorean
        ? `ì¢‹ì€ ì§ˆë¬¸ì´ì—ìš”! ğŸ˜Š ì°¨ê·¼ì°¨ê·¼ ì„¤ëª…í•´ ë“œë¦´ê²Œìš”.\n\n${this.generateCoachingResponse(lastMessage)}`
        : `Great question! Let me explain step by step.\n\n${this.generateCoachingResponse(lastMessage)}`,
      'gemini-3-flash': isKorean
        ? `ë¶„ì„ ê²°ê³¼ë¥¼ ê³µìœ í•´ ë“œë¦´ê²Œìš”.\n\n**í•µì‹¬ í¬ì¸íŠ¸:**\n1. í˜„ì¬ ìƒí™© íŒŒì•…\n2. ê°œì„  ë°©í–¥ ì œì‹œ\n3. ì‹¤í–‰ ê³„íš ìˆ˜ë¦½\n\nìì„¸í•œ ë‚´ìš©ì€ ì•„ë˜ì—ì„œ í™•ì¸í•˜ì„¸ìš”.`
        : `Here's my analysis.\n\n**Key Points:**\n1. Current situation assessment\n2. Improvement directions\n3. Action plan\n\nSee details below.`,
    };

    return {
      content: responses[model],
      model,
      usage: {
        promptTokens: Math.floor(lastMessage.length / 4),
        completionTokens: Math.floor(responses[model].length / 4),
        totalTokens: Math.floor((lastMessage.length + responses[model].length) / 4),
      },
      latencyMs: Date.now() - startTime,
    };
  }

  /**
   * ì½”ì¹­ ì‘ë‹µ ìƒì„± (ì‹œë®¬ë ˆì´ì…˜)
   */
  private generateCoachingResponse(message: string): string {
    if (/ê³„íš|ìŠ¤ì¼€ì¤„|ì¼ì •/.test(message)) {
      return 'í•™ìŠµ ê³„íšì„ ì„¸ì›Œë³¼ê²Œìš”. í•˜ë£¨ì— 30ë¶„ì”©, ì£¼ 5ì¼ í•™ìŠµí•˜ë©´ í•œ ë‹¬ ì•ˆì— ëª©í‘œë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆì–´ìš”!';
    }
    if (/ì–´ë ¤|í˜ë“¤|ëª¨ë¥´/.test(message)) {
      return 'ì–´ë ¤ìš´ ë¶€ë¶„ì´ ìˆìœ¼ë©´ í•¨ê»˜ í•´ê²°í•´ ë´ìš”. í•˜ë‚˜ì”© ì°¨ê·¼ì°¨ê·¼ í’€ì–´ê°€ë©´ ë¶„ëª… í•  ìˆ˜ ìˆì–´ìš”!';
    }
    if (/ì§„ë„|ì–¼ë§ˆë‚˜/.test(message)) {
      return 'ì§€ê¸ˆê¹Œì§€ ì •ë§ ì˜ í•˜ê³  ìˆì–´ìš”! í˜„ì¬ ì§„ë„ëŠ” ì•½ 40% ì •ë„ì´ê³ , ì´ ì†ë„ë¼ë©´ ëª©í‘œ ë‹¬ì„±ì´ ì¶©ë¶„íˆ ê°€ëŠ¥í•´ìš”.';
    }
    return 'ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”. í•¨ê»˜ í•™ìŠµ ì—¬ì •ì„ ê±¸ì–´ê°ˆê²Œìš”! ğŸ’ª';
  }

  /**
   * ëŒ€ê¸° ìœ í‹¸ë¦¬í‹°
   */
  private sleep(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  /**
   * í†µê³„ ì¡°íšŒ
   */
  getStats(): { requestCount: number } {
    return { requestCount: this.requestCount };
  }
}

// ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
let llmClientInstance: LLMClient | null = null;

export function getLLMClient(config?: Partial<LLMClientConfig>): LLMClient {
  if (!llmClientInstance) {
    llmClientInstance = new LLMClient(config);
  }
  return llmClientInstance;
}
